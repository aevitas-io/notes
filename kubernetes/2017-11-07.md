# 11:40
Back.

Concepts. Let's go.

https://kubernetes.io/docs/concepts/architecture/nodes/#management  
Kubernetes doesn't make nodes, nodes are machines created for the cluster
by some other provider. "Creating" a node in Kubernetes just means
creating an object that references it so k8s is aware of it. I wonder
how much bootstrapping is required to get a node ready to be managed...

https://kubernetes.io/docs/concepts/architecture/nodes/#node-controller  
This assigns the CIDR block to the node.
This also ensures this node knows about all other nodes.
Is this roughly analogous to consul's gossip protocol?
This also does health monitoring.
Lots of details here about how k8s handles rebalancing work if there is a
network partition.
Also some description of how you can make a node refuse or prefer some kind
of work "taints" and "tolerations". I can see there is more about this in a
future "concepts" section... resisting the urge to jump ahead, I'm already
several jumps out from my systematic consumption of these docs.

https://kubernetes.io/docs/concepts/architecture/nodes/#self-registration-of-nodes  
Neat. I assume self registration manages creating the node object in the
cluster.

https://kubernetes.io/docs/concepts/architecture/nodes/#manual-node-administration  
Some nitty gritty details like how to cordon off nodes as an administrator
if you are upgrading them. Something called a DaemonSet doesn't respect
this... ignoring for now.

https://kubernetes.io/docs/concepts/architecture/nodes/#node-capacity  
Some notes here about how to bootstrap a node with reservations on what its
capacity is.

https://kubernetes.io/docs/concepts/architecture/nodes/#api-object  
Makes sense to me.

# 12:05
Now to understand how master/node communication works.

https://kubernetes.io/docs/concepts/architecture/master-node-communication/#overview  
The "master" in this documentation refers to the api cluster.

# 12:10
@iros called.

# 12:40
Talking to Tara.

# 12:50
Scheduling with client.

# 13:00
Let's try this again.

https://kubernetes.io/docs/concepts/architecture/master-node-communication/#cluster---master  
Nodes can communicate with the api server as well, but they have to be
provisioned with the root certificate and credentials to do so.

Pods can communicate with the api server as well, "by leveraging a service
account" (not sure what that is) that makes k8s automatically inject the
root cert and a bearer token. There is a `kubernetes` service present in
all namespaces that proxies back to the api server.

Master components communicate insecurely over localhost with the api server
See https://github.com/kubernetes/kubernetes/issues/13598 for fixing this.

https://kubernetes.io/docs/concepts/architecture/master-node-communication/#master---cluster  
https://kubernetes.io/docs/concepts/architecture/master-node-communication/#apiserver---kubelet  
The api server can communicate with kubelets running on nodes to do the
following:
fetch logs for pods
using kubectl to run pods
providing port forwarding for the kubelet (...configuring it?)

The kubelet running on each node has a HTTPS endpoint. That's what the
apiserver is talking to. By default it doesn't validate the kubelet cert.
There is some setup required to fix this but I am unclear what, at this
point. Note to self: investigate further.

https://kubernetes.io/docs/concepts/architecture/master-node-communication/#apiserver---nodes-pods-and-services  
The api server communicates directly with nodes/pods/services over http.
You can use https but no validation of certs will occur.

https://kubernetes.io/docs/concepts/architecture/master-node-communication/#ssh-tunnels  
GCE uses SSH tunnels to secure communication from the master into the
cluster.

https://kubernetes.io/docs/concepts/architecture/cloud-controller/  
This section is missing a table of contents. Note to self: do a PR for this
later.

Cloud Controller Manager
This is a system for running cloud-specific plugins. This decouples those
plugins from kubernetes core. You can run it as an "addon" which means, I
assume, that the cloud controller runs in a container/pod/node managed
by kubernetes.

Controller managers run a loop that is constantly monitoring and trying to
make the current status of the cluster match the desired state. There are
"controller loops" for managing different aspects of the cluster.

Design
The kubernetes controller manager controls non-cloud specific operations.

The cloud controller manager controls cloud-specific operations.

...presumably stuff like security groups, load balancers, etc?

Components of the CCM
Functions of the CCM
The CCM takes control of some operations that the KCM manages, when
applicable (e.g. when your cluster is on a cloud provider)

Majority of CCM functions are derived from KCMs.
Hmm... does that mean the code has been copied?

CCM manages node, route, service and persistentvolumelabel controllers.

Node controller
- get cloud specific regions/labels/instance details
- get network address and hostname
- check cloud to see if node is gone, purge node from api server if so

Route controller
only used on GCE to ensure container-to-container communication is
possible cross-node.

Service controller
Configures load balancers when services are brought up.

PersistentVolumeLabels controller
Labels volumes so users don't have to. Important because a volume can only
be used within its own region. Pods specs shouldn't have to explicitly
choose a region when describing the volumes they need, but the scheduler
needs to know this stuff to make the volumes available.

2. Kubelet (why is this titled 2. ?)
Node controller contains the cloud-specific functionality for kublets.
Presumably this makes the kubelet implementation more straightforward.
Kubelet registers nodes with the api-server, and can't provide cloud
specific details any longer. Instead it "taints" the new node after
registering it and the CCM sees this and fixes it.

Feels a bit rube goldberg, but, neat!

3. Kubernets API server
PersistentVolumeLabels controller has moved out of the api server into
the CCM.

Plugin mechanism
Some notes about how this works. There are go interfaces for creating new
cloud controller managers.

Authorization
A big list of what permissions each controller needs. Not committing this
to memory yet.

Vendor Implementations
Looks like DO, Oracle, Azure, GCE and AWS have all taken ownership of
their controller managers. Neat.

Cluster Administration
Link to the tasks section on how to run the CCM. Don't need this yet.

# 13:50
https://kubernetes.io/docs/concepts/api-extension/custom-resources/  
A resource is an endpoint in the Kubernetes API that has a collection of
API Objects of a certain kind... (roy fielding acolytes flip out here)

Custom resources are not necessarily available in every cluster.

https://kubernetes.io/docs/concepts/api-extension/custom-resources/#custom-controllers  
Without a controller, custom resources just let ya store and retrieve stuff.
Add in a controller and you have a declarative api that is constantly
driving toward a desired state.

https://kubernetes.io/docs/concepts/api-extension/custom-resources/#customresourcedefinitions  
A built in API to make making more resources easier.

https://kubernetes.io/docs/concepts/api-extension/custom-resources/#api-server-aggregation  
Makes sense. Not applicable to me for a long time, I hope.

# 13:55
Stretch.
Drink water.

# 14:00
https://kubernetes.io/docs/concepts/containers/images/  
Container images are created and pushed to a registry before they are
deployed to kubernetes. The image property of a container matches docker
syntax.

https://kubernetes.io/docs/concepts/containers/images/#updating-images  
The node-level kubelet is responsible for fetching images. By default it
will not fetch an image it already has. If you want it to fetch the image
every time it is doing a deployment, set the image pull policy to always.

This is the thing I was confused about on day one! Makes perfect sense now.

If no tag is specified on an image, it is assumed to be using :latest.

https://kubernetes.io/docs/concepts/containers/images/#using-a-private-registry  
Private registries need credentials.
Some notes here about approaches to providing them.

https://kubernetes.io/docs/concepts/containers/images/#using-google-container-registry  
Just works.

https://kubernetes.io/docs/concepts/containers/images/#using-aws-ec2-container-registry  
IAM roles to the rescue.

https://kubernetes.io/docs/concepts/containers/images/#using-azure-container-registry-acr  
Node config required by cluster manager, aka, me. Boo.

https://kubernetes.io/docs/concepts/containers/images/#configuring-nodes-to-authenticate-to-a-private-repository  
This section is an ops jungle. Not going to commit this to memory right now.

https://kubernetes.io/docs/concepts/containers/images/#pre-pulling-images  
You can configure a kubelet to never pull an image so you can put your own
on it locally. Gross. I hope I never need to use this.

https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod  
This is the correct approach for pulling from private repos.

https://kubernetes.io/docs/concepts/containers/images/#use-cases  
Don't need to know use-cases, moving on.

# 14:15
https://kubernetes.io/docs/concepts/containers/container-environment-variables/#container-environment  
Makes sense to me.

https://kubernetes.io/docs/concepts/containers/container-environment-variables/#cluster-information  
There is an environment variable with the ip and port of every service that
was active at the time the container was created. Yeck. If the DNS addon is
enabled you can use DNS tho. Yes please.

https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/  
https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#overview  
*plugs ears* http://autopilotpattern.io/
No... actually I'm going to be pragmatic and just use the features of the
scheduler ...I think.

https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks  
PostStart
Executes after container is created, but maybe not before it's started.

PreStop
Executes right before termination.

https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#hook-handler-implementations  
Containers need to register their desire to run lifecycle handlers?
...not sure how. Looks like you can have k8s notify the lifecycle via
exec and http tho.

https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#hook-handler-execution  
Hook handler calls are sync within the pod. As before, PostStart and
and ENTRYPOINT in the container may be called out of sync. If the hook
hangs, the container will never reach a running state. Same story for
PostStop except it will get stuck Terminating until the grace period ends.

https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#hook-delivery-guarantees  
Apparently the hook delivery is not guaranteed to only happen once.
...so every hook author needs to write their hooks defensively to handle
that. First thought is no thanks on this feature. Presumably this doesn't
matter if google is getting by just fine tho.

https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#debugging-hook-handlers  
Hook output is available by running `kubectl describe pod <name>`. I wonder
if it is possible to externalize stuff happening in the autopilotpattern
to elevate a scheduler agnostic lifecycle?

# 14:30
More about Pods. On a roll today.

https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/#understanding-pods  
https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/#how-pods-manage-multiple-containers  
https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/#networking  
Yup. This is all review now.

https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/#working-with-pods  
I probably won't manage Pods, they'll just be created/modified/destroyed as
a side effect of my work. To the extent I actively control pods it'll be
through the orchestration of controllers to do it for me.

https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/#pods-and-controllers  
Some controller types, Deployment, StatefulSet & DaemonSet are common
controllers to generate pods from templates I author.

https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/#pod-templates  
Makes sense to me.

https://kubernetes.io/docs/concepts/workloads/pods/pod/#what-is-a-pod  
The shared context of a pod is a linux namespace, cgroups, and other facets
of isolation. Same as docker containers.

Containers in a pod can all find each-other on localhost.

In terms of Docker, a pod is a group of docker containers with a shared
namespace and volumes. PIDs are not shared (because docker does not
support it).

Each Pod gets a UID. If the pod dies and is rescheduled the previous PID
is not re-used, an identical pod is created with a new UID.

https://kubernetes.io/docs/concepts/workloads/pods/pod/#motivation-for-pods  
https://kubernetes.io/docs/concepts/workloads/pods/pod/#management  
Pods are for management. Sure yah got a bunch of containers and you can
schedule them anywhere but what if it just makes the whole thing easier to
ensure some containers are always sitting right next to each-other and will
live and die together? Makes sense.

https://kubernetes.io/docs/concepts/workloads/pods/pod/#resource-sharing-and-communication  
Since pods all share the same network you have to be careful with ports.

https://kubernetes.io/docs/concepts/workloads/pods/pod/#uses-of-pods  
Some rough details about the usage of pods.
Note to self: read this
http://blog.kubernetes.io/2015/06/the-distributed-system-toolkit-patterns.html  

https://kubernetes.io/docs/concepts/workloads/pods/pod/#alternatives-considered  
But why can't we just run multiple processes in a single container, the
world asked. Cause that's dumb. Make your container easy to monitor.
Make your container easy to version (it's only doing one thing), make your
container easy to start/stop/etc (no signal propagation). Efficiency. Let
the infrastructure deal with this crap, devs.

https://kubernetes.io/docs/concepts/workloads/pods/pod/#durability-of-pods-or-lack-thereof  
Pods facilitate:
Schedule and controller portability (I assume this means they abstract over docker/rkt)
pod-level operations (give a single, first class unit to work with)

I'm on board with pods but these listed reasons make no sense to me:
decouple pod lifetime from controller lifetime
decouple controllers and services - endpoint controller just watches pods
clean composition kubelet-level vs cluster-level functionality
ha apps which want new pods in place before old pods are destroyed

Ignoring this for now.

https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods  
https://kubernetes.io/docs/concepts/workloads/pods/pod/#force-deletion-of-pods  
Some examples showing how k8s manages shutting down Pods as one unit.
I'm glad this stuff exists already--this is a few abstractions lower than
I currently care to traverse.

https://kubernetes.io/docs/concepts/workloads/pods/pod/#privileged-mode-for-pod-containers  
Sudo containers. Good to know. Hopefully I won't need this.

# 15:05
Talking to @mattsurabian about how Kubernetes is basically Wordpress.
Yah got all kinds of controllers driving api state in an uncoordinated and
possibly conflicting fashion... feels similar to "filters" in Wordpress.

# 15:30
A bit weary of reading about Pods. Sticking with it, though.

https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/  
https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-phase  
Pending - in the system, some containers missing
Running - pod bound to node, all containers created. at least one still
running
Succeeded - all containers inside terminated on purpose
Failed - all containers inside terminated, at least one died with non-zero
Unknown - just that. unknown. usually lack of comms with host.

https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-conditions  
Makes sense to me.

https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes  
There are some standard affordances for monitoring built into k8s. You can
execute commands, run a port check over tcp, and make a http get call.
Probes can have one of three results: success/failure/unknown.

At a node level, kubelet is prepared to act on the following:
livenessProbe - is the container running?
readinessProbe - is the container ready to serve traffic?

https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#when-should-you-use-liveness-or-readiness-probes  
If your container will crash on a failure, you may not need a liveness probe
as kubelet will just restart it.

If you actively want to restart your container when a probe fails, change
the restartPolicy to onFailure or Always.

If you want to wait to send traffic to a pod until a probe succeeds, use a
readiness probe.

Draining requests on pod deletion is already handled by signals.

https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-and-container-status  
Links to api documentation. Skipping for now.

https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-lifetime  
...these docs are outdated. Moving on.

# 15:50
https://kubernetes.io/docs/concepts/workloads/pods/init-containers/  
Pods can have multiple Containers running apps. Init containers are
run before the app Containers are started.

If an Init Container fails to start, the whole pod is restarted unless
restartPolicy is never.

This seems like a hack to support the fact that developers don't handle
application startup gracefully. I'm not sure I think this should have to be
in the scheduler, but, c'est la'vie, the real world persists.

https://kubernetes.io/docs/concepts/workloads/pods/init-containers/#differences-from-regular-containers  
Init containers do not support readinessProbes, they *are* readinessProbes.
If multiple containers are specified, they are run in order sequentially
until they succeed.

https://kubernetes.io/docs/concepts/workloads/pods/init-containers/#what-can-init-containers-be-used-for  
More salvo on why you'd use these:

Run utilities you don't want in your app container for security
...like what?

Can contain custom code for setup that isn't in the app image
...seems like an app architecture problem

The application image builder and deployer roles can work independently
without the need to jointly build a single app image
...okay okay, seems like the real world, sometimes

They use Linux namespaces so that they have different filesystem views from
app Containers. Consequently, they can be given access to Secrets that app
Containers are not able to access.
...for what?

They run to completion before any app Containers start, whereas app
Containers run in parallel, so Init Containers provide an easy way to block
or delay the startup of app Containers until some set of preconditions are
met.
...show me the use-case and this one might have me beat, I suppose.

https://kubernetes.io/docs/concepts/workloads/pods/init-containers/#examples  
https://kubernetes.io/docs/concepts/workloads/pods/init-containers/#init-containers-in-use  
These all seem like the job of the application developer.

https://kubernetes.io/docs/concepts/workloads/pods/init-containers/#detailed-behavior  
Any time a pod is restarted, all init containers run again.
Init containers must be idempotent, they could be called multiple times.

https://kubernetes.io/docs/concepts/workloads/pods/init-containers/#resources  
Not committing this to memory.

https://kubernetes.io/docs/concepts/workloads/pods/init-containers/#pod-restart-reasons  
Some edge cases.

https://kubernetes.io/docs/concepts/workloads/pods/init-containers/#support-and-compatibility  
Check.

# 16:20
https://kubernetes.io/docs/concepts/workloads/pods/podpreset/  
A resource for injecting properties into pods so that pod template authors
don't need to specify everything. I prefer explicitness, this rubs me the
wrong way.

https://kubernetes.io/docs/concepts/workloads/pods/podpreset/#how-it-works  
Could this sort of thing be supported at the pod spec level by allowing
external references?

Not into this but I'm sure if I weren't operating in an ivory tower there
would be good cause for it.

# 16:25
https://kubernetes.io/docs/concepts/workloads/pods/disruptions/  
Let's see how pods can blow up...

https://kubernetes.io/docs/concepts/workloads/pods/disruptions/#voluntary-and-involuntary-disruptions  
So, every reason an application could fail is one...
(VM failure, kernel panic, etc, etc, etc)
Deleting a deployment.
Updating template and causing a restart.
Deleting a pod by accident.
Shutting down a node for repairs.

https://kubernetes.io/docs/concepts/workloads/pods/disruptions/#dealing-with-disruptions  
Ensure pod requests resources needed so it doesn't run out of them.
Replicate your app if you need HA.
Ensure app runs across availability zones.

https://kubernetes.io/docs/concepts/workloads/pods/disruptions/#how-disruption-budgets-work    
https://kubernetes.io/docs/concepts/workloads/pods/disruptions/#pdb-example  
https://kubernetes.io/docs/concepts/workloads/pods/disruptions/#how-to-perform-disruptive-actions-on-your-cluster  
Not going to read this now, but, effectively, you can specify a level of
disruption you'll accept to ensure that cluster managers can make a best
effort to respect that.

Yaaay, I made it out of the pod section.

# 16:30
https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/  
https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/#how-to-use-a-replicaset  
https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/#when-to-use-a-replicaset  
I won't use this directly--I'll get these from the Deployment controller.
Carrying on anyway because I want to understand the "base" primitive.

https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/#example  
Makes sense to me.

https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/#writing-a-replicaset-spec  
https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/#pod-template  
Makes sense to me.

https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/#pod-selector  
This confirms my question much earlier. You can have ReplicaSets with
overlapping selectors and you just have to deal with it. Eesh. I hope those
hashes I saw at the Deployment level prevent these types of collisions.

https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/#labels-on-a-replicaset  
https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/#replicas  
Makes sense to me.

https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/#working-with-replicasets  
https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/#deleting-a-replicaset-and-its-pods  
Makes sense to me.

https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/#deleting-just-a-replicaset  
Yikes, you can delete a ReplicaSet and it won't delete the running Pods.
You can then introduce a new ReplicaSet and it will take ownership of the
existing Pods but not update them if the pod template changed. Again, I hope
the higher level Deployment construct deals with this more intuitively (to
me anyway).

https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/#isolating-pods-from-a-replicaset  
You can pull a pod out of a ReplicaSet by changing its labels. The system
will then automatically replace it. Neat.

https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/#scaling-a-replicaset  
Makes sense to me.

https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/#replicaset-as-an-horizontal-pod-autoscaler-target  
Neeaaaat.

# 16:45
https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/  
Skipping, has been replaced by ReplicaSet

https://kubernetes.io/docs/concepts/workloads/controllers/deployment/  
Declarative updates for Pods and ReplicaSets together.
Directly managing ReplicaSets that a Deployment creates is a footgun. Check.

https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#use-case  
Update the PodTemplate and gracefully update all pods. Check.
Rollback. Check.
Scale the deployment to facilitate more load. Check.
Pause the deployment. Check.
Watch the status of a deployment as a whole. Check.
Clean up older ReplicaSets. ...interesting, not sure what this.

https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#creating-a-deployment  
Seems it is possible to create a selector for a deployment that does not
match the label in the pod template. I assume that would fail.

`matchLabels` is sugar for `matchExpression`.

Appending `--record` to a kubectl command records it in annotations.

You can see the underlying replicasets with `kubectl get rs`

# 17:00
Wrapped for the day.
