# 15:45
https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable  
By default, Pods can consume all the capacity on a Node. Kubelet has a
feature that allows for setting aside room to run the daemones required to
keep a system up at all. The feature is called `Node Allocatable`.

Allocatable on a Node is the amount of resources available for Pods.
Presently, CPU/memory/ephemeral-storage are supported.

https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/#enabling-qos-and-pod-level-cgroups  
To enable this feature, usage of the flag `--cgroups-per-qos` is required.
This is the default.

https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/#configuring-a-cgroup-driver  
Configuration happens using the `--cgroup-driver` flag. Supported values are
`cgroupfs` or `systemd`. The selection must match the container runtime.

https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/#kube-reserved  
`--kube-reserved` is meant to capture cgroup reservation for kubelet, the
container runtime, the node problem detector, etc. Not meant to reserve
resources for daemons run in Pods.

Jumping to blog post that describes how to decide the resources to reserve:
http://blog.kubernetes.io/2016/11/visualize-kubelet-performance-with-node-dashboard.html  

https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/#system-reserved  
https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/#eviction-thresholds  
https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/#enforcing-node-allocatable  
https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/#general-guidelines  
I need to read more about control groups before I can really use this.

One thing I learned is that, I think, a Pod's qosClass relates to which
cgroup it is placed in.

# 16:00
https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/  
Some critical parts of K8s run in the cluster itself. We don't want them to
be evicted.

https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/#rescheduler-guaranteed-scheduling-of-critical-add-ons  
Not quite sure what this is trying to say, but basically there is a concept
of a "rescheduler" that re-evaluates where Pods are located because the
scheduler only evaluates this at the time of creation. A rescheduler might
choose to move things around.

# 16:05
https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/  
https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/#before-you-begin  
Not sure if Minikube supports the networking required for this test. Going
to try anyway.

https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/#create-an-nginx-deployment-and-expose-it-via-a-service  
https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/#test-the-service-by-accessing-it-from-another-pod  
Create an nginx deployment, give it a service spin up a pod and communicate
with it. Works fine.

https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/#limit-access-to-the-nginx-service  
https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/#assign-the-policy-to-the-service  
Now add a NetworkPolicy so only Pods with specific labels can communicate.

https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/#test-access-to-the-service-when-access-label-is-not-defined  
Test communication with a Pod not using the correct labels.
...works fine.

Okay, so, Minikube doesn't support this by default. Let's see if I can get
a CNI set up.

...found this:
http://cilium.readthedocs.io/en/stable/gettingstarted/#getting-started-using-kubernetes  

Giving it a shot.
```
minikube start --network-plugin=cni
kubectl create -f https://raw.githubusercontent.com/cilium/cilium/HEAD/examples/kubernetes/cilium.yaml
kubectl get pod --namespace kube-system -w
```

...okay Cilium is up and running. I have only the lightest of understanding
for what it is doing though. Nevertheless, trying again...

https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/#test-access-to-the-service-when-access-label-is-not-defined  
Test communication with a Pod not using the correct labels.
Doesn't work (as expected!).

https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/#define-access-label-and-test-again  
Test communication with Pod using the correct labels.
Works as expected!

Neat. Glad I don't need this level of control, yet.

# 16:30
https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/  
https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/  
Looks like earlier versions of Kubelet didn't support a config file, but
soon it will be possible to use a config file and ConfigMaps to do so.
Neat.

# 16:35
https://kubernetes.io/docs/tasks/administer-cluster/calico-network-policy/  
https://kubernetes.io/docs/tasks/administer-cluster/cilium-network-policy/  
https://kubernetes.io/docs/tasks/administer-cluster/kube-router-network-policy/  
https://kubernetes.io/docs/tasks/administer-cluster/romana-network-policy/  
https://kubernetes.io/docs/tasks/administer-cluster/weave-network-policy/  
These docs show how to install various container networking interfaces. I am
skipping for now as I have not yet chosen which makes the most sense for me.

Breaking for now.
