# 14:20
https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/  
https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#before-you-begin  
Check.

https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#define-a-liveness-command  
Many applications running for a long time ultimately crash and need to be
restarted. K8s provides support for this with liveness probes that make the
system aware when the application goes into a failed state. I love the
pragmatism this speaks to.

This example probes for liveness every 5 seconds and gives the app 5 seconds
to start up. It's set up to break itself after the first 30 seconds.

Spun it up and confirmed it works. Makes sense to me.

https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#define-a-liveness-http-request  
This example probes every 3 seconds and waits 3 seconds for startup.
Success for a HTTP probe is a status more than or equal to 200 and less
than 400. Spun it up and confirmed it works. Makes sense to me.

https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#define-a-tcp-liveness-probe  
This examples adds a readiness probe that is sent after startup (5 seconds)
and runs every 10 seconds. It also shows a liveness probe that runs every 20
seconds.

https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#use-a-named-port  
It's possible to name a port using the ContainerPort resource and re-use it
in all pods.

https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#define-readiness-probes  
This describes the purpose of a readiness probe. This is review for me now.

https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes  
Probes can be configured with more granularity than shown in the examples.
You can state how many failures or successes are required before a state
change occurs.

https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/#before-you-begin  
https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/#add-a-label-to-a-node  
Shows how to label nodes and view them with `--show-labels`.  Makes sense to
me.

https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/#create-a-pod-that-gets-scheduled-to-your-chosen-node  
Shows how to add a nodeSelector to force a specific type of node to be
chosen. Makes sense to me.

# 14:25
https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-initialization/#before-you-begin  
https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-initialization/#create-a-pod-that-has-an-init-container  
Shows how to use init containers. This is review for me. Makes sense.

https://kubernetes.io/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/#before-you-begin  
https://kubernetes.io/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/#define-poststart-and-prestop-handlers  
This is review--showing how to add postStart and preStop handlers.

https://kubernetes.io/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/#discussion  
postStart is executed immediately after a Container is started. We don't
know if this is called before the ENTRYPOINT, though. This means that
postStart is async relative to the Container but K8s is not... it will not
mark a container ready until the the the postStart handler completes.

Same story for preStop, Container will not be forcibly killed until the
handler completes.

# 14:30
https://kubernetes.io/docs/tasks/debug-application-cluster/debug-pod-replication-controller/  
First step in debugging a pod is to describe it.
Are Containers running?
What about recent restarts?

https://kubernetes.io/docs/tasks/debug-application-cluster/debug-pod-replication-controller/#my-pod-stays-pending  
Pod stuck in pending means it can not been scheduled.
Some common reasons:
https://kubernetes.io/docs/tasks/debug-application-cluster/debug-pod-replication-controller/#insufficient-resources  
No Nodes with enough CPU/RAM to support.
...clean up resources or add more nodes.

https://kubernetes.io/docs/tasks/debug-application-cluster/debug-pod-replication-controller/#using-hostport  
If you have a hostPort defined, you can only one one of this workload per
node. Check that.

https://kubernetes.io/docs/tasks/debug-application-cluster/debug-pod-replication-controller/#my-pod-stays-waiting  
Pod stuck in waiting means it's been scheduled but can't run.
Most common case is a failure to get the container image.
Run a manual docker pull <image> to see if you can get it (assuming it
isn't on a private registry, anyway)

https://kubernetes.io/docs/tasks/debug-application-cluster/debug-pod-replication-controller/#my-pod-is-crashing-or-otherwise-unhealthy  
Check the logs.
kubectl logs <pod-name> <container-name>

If it crashed, check the previous container's logs with:
kubectl logs --previous <pod-name> <container-name>

...try running commands in the container with
kubectl exec <pod-name> -c <container-name> -- command here
^ -c is optional, if your pod has one container

# 14:35
Chatting with Tara.

# 15:05
https://kubernetes.io/docs/tasks/debug-application-cluster/debug-service/#conventions  
This describes some conventions so the context the exmaple commands are
running in is clear.

https://kubernetes.io/docs/tasks/debug-application-cluster/debug-service/#running-commands-in-a-pod  
When debugging services you often want to do it from the perspective of a
Pod in the cluster. To do that, run a busybox pod.
```
kubectl run -it --rm --restart=Never busybox --image=busybox sh
```

https://kubernetes.io/docs/tasks/debug-application-cluster/debug-service/#setup  
Spin up a deployment to create a service for.

https://kubernetes.io/docs/tasks/debug-application-cluster/debug-service/#does-the-service-exist  
Check if the thing is on first, kid. Is there a Service created yet?
kubectl get svc hostnames (from machine managing cluster)
`wget -qO- <service-name>` (from pod)

If those fail... make the service yo.

https://kubernetes.io/docs/tasks/debug-application-cluster/debug-service/#does-the-service-work-by-dns  
Okay, so the service exists. Does it work via DNS?
Use nslookup to find out.
```
nslookup hostnames
```

...if that fails, maybe your test Pod (or the pod that can't reach the
service you are trying to manage) is in the wrong namespace. Be more
specific, add the namespace qualifier:

```
nslookup hostnames.default
```
(default is the default namespace, obviously)

...if that works, decide if you want the Pod that needs to consume this
service to run in the same namespace, or if it needs to be updated to a
more qualified name.

...if that doesn't work, try a fully-qualified lookup:
```
nslookup hostname.default.svc.cluster.local
```
...if that works but lesser qualified requests don't work, something is
amiss with your /etc/resolv.conf. Here is an example:

```
nameserver 10.0.0.10
search default.svc.cluster.local svc.cluster.local cluster.local example.com
options ndots:5
```

`nameserver` should be your cluster's DNS Service. kubelet controls this for
all pods with the flag `--cluster-dns`

`search` should include the suffixes you are trying to omit.

`options` must set ndots high enough for these to be used at all. k8s sets
this to 5 which is high enough to cover the deepest level of nesting it
creates.

https://kubernetes.io/docs/tasks/debug-application-cluster/debug-service/#does-any-service-exist-in-dns  
If all that failed, check to see if -any- DNS exists. Try
```
nslookup kubernetes.default
```

If this fails, debug DNS itself.

https://kubernetes.io/docs/tasks/debug-application-cluster/debug-service/#does-the-service-work-by-ip  
Assuming DNS works but the Service is still broken... see if your Service
works at all. Inside a node, test the IP directly using whatever manner is
applicable for the Service, e.g.
```
curl 10.0.1.175:80
```

If the Service is up you should get responses you expect.

https://kubernetes.io/docs/tasks/debug-application-cluster/debug-service/#is-the-service-correct  
More sanity checking. Read the configuration for the Service itself closely.

```
kubectl get service hostnames -o json
```

Some things to validate:
Is the port you are trying to access in spec.ports[]?
Is the targetPort correct for your Pods?
If you meant it to be a numeric port, is it a number or a string?
If you meant it to be a named port, do your Pods use the same?
Is the port’s protocol the same as the Pod’s?

https://kubernetes.io/docs/tasks/debug-application-cluster/debug-service/#does-the-service-have-any-endpoints  
So the Service exists and the DNS resolves. Is it selecting the right Pods?
Check your Endpoints.

`kubectl get endpoints hostnames`

If that shows IPs for your Pods...

https://kubernetes.io/docs/tasks/debug-application-cluster/debug-service/#are-the-pods-working  
... are your Pods working?

Try communicating with the Service that is meant to be exposed directly from
within another Pod.

https://kubernetes.io/docs/tasks/debug-application-cluster/debug-service/#is-the-kube-proxy-working  
If you are this far down the rabbit hole you have a Service with valid
Endpoints and Pods that can serve data. At this point, we're down to
networking itself.

From a node, see if kube-proxy is running. Check the logs with journalctl or
/var/logs/kube-proxy.log.

For manual setups, lacking the `conntrack` binary can cause issues.

If kube-proxy is running, is it writing to iptables?

https://kubernetes.io/docs/tasks/debug-application-cluster/debug-service/#iptables  
Run `iptables-save` to see the current iptables state and grep for the
service you care about.

> There should be 1 rule in KUBE-SERVICES, 1 or 2 rules per endpoint in
KUBE-SVC-(hash) (depending on SessionAffinity), one KUBE-SEP-(hash) chain
per endpoint, and a few rules in each KUBE-SEP-(hash) chain. The exact
rules will vary based on your exact config (including node-ports and
load-balancers).

Find the port kube-proxy is using for your service in the iptables rules
and try to use it directly against localhost. If *that* fails, restart
kube-proxy with verbosity set to 4 (-V 4).

https://kubernetes.io/docs/tasks/debug-application-cluster/debug-service/#a-pod-cannot-reach-itself-via-service-ip  
If Pods are connected with a bridge network and the kube-proxy `hairpin`
mode isn't set correctly, Pods may not be able to reach themselves using
their Service VIP.

--

That was incredibly well written.
Taking a break.

# 16:45
https://kubernetes.io/docs/tasks/debug-application-cluster/debug-cluster/  
Listing your cluster
All nodes present? All in ready state?

Looking at logs
Use journalctl for systemd hosts.

Master
/var/log/kube-apiserver.log
/var/log/kube-scheduler.log
/var/log/kube-controller-manager.log

Worker Nodes
/var/log/kubelet.log
/var/log/kube-proxy.log

A general overview of cluster failure modes
- VM shutdowns
- Network partitions
- Software crashes
- Data loss
- Operator error

Beyond this the doc becomes so general it doesn't seem worth reading.
Moving on...

# 16:50
https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application/#diagnosing-the-problem  
Pods, Replication Controllers or Service?

https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application/#debugging-pods  
Describe the Pod. Check the state of it and the containers.

https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application/#my-pod-stays-pending  
This is review at this point. Moving on.

https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application/#my-pod-stays-waiting  
This is review at this point. Moving on.

https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application/#my-pod-is-crashing-or-otherwise-unhealthy  
This is review at this point. Moving on.

https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application/#my-pod-is-running-but-not-doing-what-i-told-it-to-do  
Compare your yaml to the one stored in the api server. Maybe something was
mistyped. Also, try validating the configuration with `kubectl --validate`.

https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application/#debugging-replication-controllers  
Makes sense.

https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application/#debugging-services  
https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application/#my-service-is-missing-endpoints  
https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application/#network-traffic-is-not-forwarded  
Just finished an incredibly thorough doc on this specific point. Moving on.

# 16:55
https://kubernetes.io/docs/tasks/debug-application-cluster/debug-stateful-set/  
This doc is basically empty. Next.

https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application-introspection/  
This is all review. Moving on.

https://kubernetes.io/docs/tasks/debug-application-cluster/local-debugging/  
This documents using a tool called telepresence and it has its own tutorial
that seems much more thorough. Using that.

https://www.telepresence.io/tutorials/kubernetes  
This tool lets your laptop serve traffic for a service in a remote
Kubernetes cluster.

When you start this, it swaps one of the Pods in the remote cluster with one
that proxies traffic back to your laptop. When you shut telepresence down it
restores the old Pod.

Pretty rad. Definitely going to look at this more closely soon.

# 17:05
https://kubernetes.io/docs/tasks/access-kubernetes-api/http-proxy-access-api/  
This is review. Moving on.

https://kubernetes.io/docs/tasks/access-kubernetes-api/extend-api-custom-resource-definitions/  
https://kubernetes.io/docs/tasks/access-kubernetes-api/extend-api-custom-resource-definitions/#create-a-customresourcedefinition  
https://kubernetes.io/docs/tasks/access-kubernetes-api/extend-api-custom-resource-definitions/#create-custom-objects  
Some tools for making new k8s resources. Reminds me of Wordpress.

https://kubernetes.io/docs/tasks/access-kubernetes-api/extend-api-custom-resource-definitions/#delete-a-customresourcedefinition  
Removing a resource definition removes all objects of that used it.

https://kubernetes.io/docs/tasks/access-kubernetes-api/extend-api-custom-resource-definitions/#finalizers  
The first delete of any object sets `metadata.deletionTimestamp` and then
controllers check to see if there are any 'finalizers' on the resource that
need to be processed first. Once all finalizers are complete, the object is
removed.

There are no details here about how finalizers are executed, in what order,
or how to author your own finalizers. Useful to know that finalizers exist,
I guess.

https://kubernetes.io/docs/tasks/access-kubernetes-api/extend-api-custom-resource-definitions/#validation  
It's possible to have automated validation of resources using the OpenAPI v3
schema format. This is currently alpha.

# 17:15
https://kubernetes.io/docs/tasks/access-kubernetes-api/extend-api-third-party-resource/  
https://kubernetes.io/docs/tasks/access-kubernetes-api/migrate-third-party-resource/  
Deprecated. Skipping.

# 17:20
https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/#overview  
Every k8s cluster has a root CA. Components in the system validate their
communications using it and client certs derived from it.

https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/#trusting-tls-in-a-cluster  
The root CA cert is typically mounted in pods at
/var/run/secrets/kubernetes.io/serviceaccount/ca.crt

https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/#requesting-a-certificate  
This is a demo of how to use CFSSL to generate a TLS cert for a k8s service
accessible by DNS.

https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/#step-0-download-and-install-cfssl  
Check.

https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/#step-1-create-a-certificate-signing-request  
Makes sense to me.

https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/#step-2-create-a-certificate-signing-request-object-to-send-to-the-kubernetes-api  
Neat, you can make certificate requests using kubectl. You can see their
status with `kubectl get csr`.

https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/#step-3-get-the-certificate-signing-request-approved  
Looks like cluster administrators have to manage and respond to these
requests manually.

https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/#step-4-download-the-certificate-and-use-it  
Once it has been Approved/Issued you can get your cert with
`kubectl get csr <name> -o jsonpath='{.status.certificate}'`

https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/#approving-certificate-signing-requests  
Approving a cert:
```
kubectl certificate approve <name>
```

https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/#a-word-of-warning-on-the-approval-permission  
Some warnings about how each approval controls what can communicate with the
cluster. Doesn't seem to be a revocation option.

https://kubernetes.io/docs/tasks/tls/certificate-rotation/#before-you-begin  
K8s 1.8 includes a beat feature for rotating certs. Certs are valid for one
year by default. Note to self: this could be a pretty significant failure
point annually...

https://kubernetes.io/docs/tasks/tls/certificate-rotation/#enabling-client-certificate-rotation  
This describes how kubelets can automatically make certificate requests and
install the resultant cert in an automated way. Yes please.

# 17:40
https://kubernetes.io/docs/tasks/administer-cluster/memory-default-namespace/  
Configure default memory requests and limits for a given namespace. These
are applied when Containers and Pods do not supply their own.

https://kubernetes.io/docs/tasks/administer-cluster/memory-default-namespace/#create-a-namespace  
https://kubernetes.io/docs/tasks/administer-cluster/memory-default-namespace/#create-a-limitrange-and-a-pod  
Create a namespace for the exercise.
Create a LimitRange resource to configure defaults.
Create a Pod with no explicit limits or requests.
See the pod get the limits and requests.
Check.

https://kubernetes.io/docs/tasks/administer-cluster/memory-default-namespace/#what-if-you-specify-a-containers-limit-but-not-its-request  
If you specify a limit but not a request the request is set to match.

https://kubernetes.io/docs/tasks/administer-cluster/memory-default-namespace/#what-if-you-specify-a-containers-request-but-not-its-limit  
If you specify a request but not a limit, the limit gets the default value.

https://kubernetes.io/docs/tasks/administer-cluster/memory-default-namespace/#motivation-for-default-memory-limits-and-requests  
This is useful for managing a resource quota in a namespace.

https://kubernetes.io/docs/tasks/administer-cluster/cpu-default-namespace/  
Skipping this, it's a repeat of the last but for CPU and not memory.

# 17:50
https://kubernetes.io/docs/tasks/administer-cluster/memory-constraint-namespace/  
https://kubernetes.io/docs/tasks/administer-cluster/cpu-constraint-namespace/  
When creating a LimitRange resource you can specify default requests and
limits and you can also specify minimums and maximums. Enforcement only
occurs at Pod creation or update time.

https://kubernetes.io/docs/tasks/administer-cluster/quota-pod-namespace/  
https://kubernetes.io/docs/tasks/administer-cluster/quota-api-object/  
Quotas can be used to limit the # of any type of resource which can be
created in a given namespace.

https://kubernetes.io/docs/tasks/administer-cluster/opaque-integer-resource-node/  
Deprecated. Skipping.

https://kubernetes.io/docs/tasks/administer-cluster/access-cluster-services/  
All review at this point. Skimming.

# 18:05
https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/  
https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#before-you-begin  
Learning best practices for securing a cluster. Ready.

https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#controlling-access-to-the-kubernetes-api  
https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#use-transport-level-security-tls-for-all-api-traffic  
Most all traffic is secured via TLS.
...except master components that communicate insecurely over localhost

https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#api-authentication  
API authorization is controlled by service accounts / bearer tokens that are
placed on in all containers.

https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#api-authorization  
RBAC.
If you disallow a user from creating a Pod but allow creating a Deployment,
the Deployment can create Pods on their behalf. Think carefully about how to
manage these types of control.

https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#controlling-the-capabilities-of-a-workload-or-user-at-runtime  
ResourceQuotes control, roughly, the capacity of a namespace.
LimitRanges control min/max size of some resources.

https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#controlling-what-privileges-containers-run-with  
PodSecurityPolicy can control what Pods can do.

https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#restricting-network-access  
Control how freely components can communicate with NetworkPolicy resources.

https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#controlling-which-nodes-pods-may-access  
Taints and Tolerations, Affinity and Anti-Affinity can help administrators
control what can be scheduled where.

https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#protecting-cluster-components-from-compromise  
https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#restrict-access-to-etcd  
Write access to etc is effectively root for the cluster. It controls the
content of the API.

https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#enable-audit-logging  
Enable audit logging.

https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#restrict-access-to-alpha-or-beta-features  
Restrict alpha/beta features.

https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#rotate-infrastructure-credentials-frequently  
Rotate creds frequently. If external systems use service accounts, be even
more cautious here.

https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#review-third-party-integrations-before-enabling-them  
Third party integrations can alter things in unexpected ways. Be sure to
reivew them.

https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#encrypt-secrets-at-rest  
Encrypt secrets at rest. Alpha support in 1.7 for ensuring etcd backups do
not have secrets available in plain text.

# 18:15
https://kubernetes.io/docs/tasks/administer-cluster/namespaces/  
https://kubernetes.io/docs/tasks/administer-cluster/namespaces/#before-you-begin  
Ready to dive into namespaces.

https://kubernetes.io/docs/tasks/administer-cluster/namespaces/#viewing-namespaces  
Yup, `kubectl get namespaces` or `kubectl get ns`

https://kubernetes.io/docs/tasks/administer-cluster/namespaces/#creating-a-new-namespace  
Creating a namespace is just like anything else.

https://kubernetes.io/docs/tasks/administer-cluster/namespaces/#deleting-a-namespace  
Removing a namespace will delete everything in it!

https://kubernetes.io/docs/tasks/administer-cluster/namespaces/#subdividing-your-cluster-using-kubernetes-namespaces  
https://kubernetes.io/docs/tasks/administer-cluster/namespaces/#understanding-the-motivation-for-using-namespaces  
Namespaces are handy for scoping names and providing groups to control
security settings, quotas, etc for a different users of a cluster.

https://kubernetes.io/docs/tasks/administer-cluster/namespaces/#understanding-namespaces-and-dns  
Namespace is a part of DNS entries.
<service>.<namespace>.svc.cluster.local

https://kubernetes.io/docs/tasks/administer-cluster/namespaces-walkthrough/  
This is review at this point. Skimming.

# 18:25
https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/  
`kubectl drain <node-name>` safely spins down a node so it can be worked on.
It allows graceful termination of pods.

https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/#draining-multiple-nodes-in-parallel  
Typically it's best to only drain a single node at a time, but you can drain
multiples concurrently and all the same graceful termination rules will be
respected.

# 18:30
All set for the night. Almost ready to deploy a cluster.
Gone to hang with Tara.
