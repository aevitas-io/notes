# 10:30
Bowl of ice cream to start the day.
Client emails.

# 10:50
Reporting for doc-reading duty.

https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#creating-a-deployment  
There are notes at the end of this that talk about how overlapping selectors
on a Deployment can cause unexpected/conflicting behavior. This speaks to
the experiment I did on 2017-11-02 on that subject...

But then there is a note immediately following that says deployments add
a `pod-template-hash` label to ensure the replicasets and their associated
pods are uniquely identifiable. So... which is it?

This is a hangnail I'm not going to mess with right now, I could probably
spend several hours trying to understand this.

https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#updating-a-deployment  
A deployment rollout is triggered only when a pod spec template is changed.
The rest of this is review at this point.

https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#rollover-aka-multiple-updates-in-flight  
Neat. If you push multiple updates out in quick succession the latest one
takes control of the previous so you don't have to finish rollout of each
"intermediate" deployment.

https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#label-selector-updates  
Looks like you shouldn't ever change the selectors in a deployment. This is
a recommendation currently but in future versions of the API it will be
enforced by validation.

https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#rolling-back-a-deployment  
https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#checking-rollout-history-of-a-deployment  
The history of each deployment update is stored in the `rollout` resource.
e.g.

`kubectl rollout history deployment/nginx-deployment`
and
`kubectl rollout history deployment/nginx-deployment --revision=2`

You can roll back any time. Scaling a deployment does not count as a
rollout. Reiterating from before, a rollout happens when you change the
pod spec.

https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#rolling-back-to-a-previous-revision  
Makes perfect sense.

`kubectl rollout undo deployment/nginx-deployment --to-revision=2`

It's staggering to me how far away nearly organization I've seen is from
this kind of flexibility.

https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#scaling-a-deployment  
Makes sense to me.

https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#proportional-scaling  
So. incredible.

Say I have 10 instances of an application running and I'm in the process of
rolling out a new version. This means there are two ReplicaSets in play, the
old and the new. If we auto-scale, should we autoscale into the new one or
the old one? This feature scales into both to mitigate risk.

https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#pausing-and-resuming-a-deployment  
You can pause a rollout to fix stuff (external to the rollout perhaps) and
then resume when ready. This saves you from making lots of deployments that
aren't needed.

Can't rollback a paused deployment--it has to complete.

https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#deployment-status  
You can watch the overall status of a deployment: `kubectl rollout status`

https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#progressing-deployment  
This means: new ReplicaSet created, scaling the ReplicaSet, scaling down old
ReplicaSet, new Pods entering ready.

https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#complete-deployment  
This means: no old ReplicaSets are left. All of the new ReplicaSets are
available.

https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#failed-deployment  
This means insufficient quota on nodes to deploy, readiness probe failures,
image pull failures, bad permissions, app runtime failures. It's possible to
set a deadline condition for a number of seconds a Deployment should
complete in. That can also indicate failure. No action happens here, it just
lets you know the condition has been violated. Pausing a Deployment pauses
the deadline count.

https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#operating-on-a-failed-deployment  
You can still interact with a failed Deployment in all the ways you can a
successful one.

https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#clean-up-policy  
By default all old ReplicaSets are retained for easy rollbacks. You can set
a revision history limit to bound this. Not sure what it means to keep the
ReplicaSet...

What is actually "saved" here? Just records in the API?

https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#use-cases  
It's easy to do canary deployments. Will read about this later.

https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#writing-a-deployment-spec  
https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#pod-template  
https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#replicas  
Makes sense to me.

https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#selector  
Latest versions of the API require this to be explicitly set vs implicitly
doing so. Glad to see this. I intended to be explicit anyway.

More notes here about overlapping selectors. Good god, there just needs to
be a single page about this that is linked to. Note to self: make an issue
that highlights this.

https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy  
Different ways to do deployments...

https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#recreate-deployment  
Kill everything first (presumably incurring downtime), then bring up the
next set.

https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#rolling-update-deployment  
Rolling update... this is the default.

https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#max-unavailable  
Controls how many pods can be taken out of the active pool while their
replacements are being brought up in the inactive.

https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#max-surge  
This controls how rapidly we can bring up new pods. For example, if the
existing deployment requires 10 pods and we set maxsurge to 3, that means
that we can spin up 3 new pods before we take any of the 10 old pods out.

https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#progress-deadline-seconds  
An optional value for awareness, set the duration a deployment should take
to complete and set a condition on the deployment (takes no action) when it
is not met.

https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#min-ready-seconds  
Haha, this controls how long a pod should be running before we really
consider it available. Presumably this guards against apps that fall
over right after being brought up.

https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#rollback-to  
Deprecated.

https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#revision-history-limit  
Control how much history we keep. All old ReplicaSets will be kept by
default, consuming resources in etcd and crowding the output of
`kubectl get rs`, if this field is not set.

https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#paused  
Are we paused or not?

# 11:50
Going for a walk in the woods to let this all sink in. Just finished the
Deployment section. Just incredible.

# 12:00
Talkin' to friends.

# 12:10
Let's go.

https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/  
I started reading about these at 1am on 2017-11-05 after a tutorial section
took me here. Then fell alseep. Let's try this again.

Beta in 1.8.
These are for managing things that have state. Databases and stuff.
We care about the ordering and uniqueness of these.

https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#using-statefulsets  
Useful for Pods that need a stable network identifier, persistent storage,
ordered and graceful deployment, scaling, termination and updates.

https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#limitations  
Storage for these pods requires a PersistentVolume or something
pre-configured by an admin. Storage for these is not removed when the pods
are removed. Needs a HeadlessService to manage network identity. This is
described later in the concepts section... waiting until I get there to
look.

https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#components  
Example config. Makes sense. Seems overly simple but I guess that is the
point.

https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-selector  
More docs about how pod selectors must be explicit now. Good.

https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-identity  
StatefulSet pods have an identity that sticks no matter what node they are
scheduled or rescheduled on.

https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#ordinal-index  
https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#stable-network-id  
The hostname of the pod is based on an ordinal index. Some example hostnames
are shown. Makes sense.

https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#stable-storage  
One volume per claim on the StatefulSet. Each Pod gets a unique volume that
stays with it based on the claim no matter where it is scheduled.

https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#deployment-and-scaling-guarantees  
StatefulSet Pods are created in order and destroyed in reverse order. Before
scaling this type of resource all earlier Pods must be running/ready. Pod
cannot be terminated until all successors are terminated.

https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-management-policies  
https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#orderedready-pod-management  
https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#parallel-pod-management  
You can relax the ordering restrictions or remove them entirely.

https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies  
There are some different update approaches supported.

https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#on-delete  
This stops updates from working until the operator deletes a pod themselves.

https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#rolling-updates  
Same as the Deployment rolling update except in order.

https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#partitions  
This lets you say update all pods with an index higher than N. If you delete
pods lower than the specified index they will be created with the same old
deployment. Interesting. Not sure what the use case is here but I'm going
to carry on.

# 12:30
https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/  
Cooooool. This set ensures some subset of pods run on -every- node.

Useful for running cluster storage, logstash, datadog, etc.

https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#writing-a-daemonset-spec  
https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#create-a-daemonset  
https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#required-fields  
https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#pod-template  
https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#pod-selector  
Makes sense to me.

Found an unfinished sentence. Creating a PR:
https://github.com/kubernetes/website/pull/6237  

https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#running-pods-on-only-some-nodes  
A selector for limiting which nodes a workload is run on. Nice.

https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#how-daemon-pods-are-scheduled  
The machines DaemonSets should run pods on are pre-selected by the node
selector. This means the scheduler doesn't need to do any work finding
nodes... we specified it already.

So, the unschedulable field on node is ignored and pods can be spun up even
when the scheduler isn't running. This has interesting implications for
bootstrapping.

Some additional details about how this interacts with taints that don't
quite make sense. I'm not going to dig here right now.

https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#communicating-with-daemon-pods  
Push data - DaemonSet Pods push updates out.
NodeIP and KnownPort - Clients can talk to the Pods by node ip and some
convention-based port.
DNS - Use a headless service to discover the DaemonSets.
Service - use a Pod selector to make a service. No control about which node
is hit though.

https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#updating-a-daemonset  
Updates happen when the Node label changes. This adds or removes Pods on
the Nodes which are appropriate. You can't change some properties of the
Pods though? Not sure what this means.

You can delete DaemonSets without removing the Pods with --cascade=false.
You can then put a new DaemonSet in place to re-own the Pods but you have
to manually destroy the Pods to get the DaemonSet to update them.

Apparently some of what I just read is false because a rolling update is
possible as of 1.6?

https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#alternatives-to-daemonset  
Some alternatives...

Just use the init system on the Node like you would in a normal sysadmin
role. You lose a few things tho:
- monitoring daemons in the same way applications are monitored
- same config/tools/etc/
- better update paths coming (potentially here, just need to read about 'em)

Opened an issue to correct that mention:
https://github.com/kubernetes/website/pull/6240  

https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#bare-pods  
Don't make individual Pods by hand.

https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#static-pods  
Some old thing that I don't want.

https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#deployments  
Deployments are good for stateless stuff but don't try to make it work
for Daemons, that's what DaemonSets are for.

# 13:05
Almost done with the workload controllers section. Wee.

https://kubernetes.io/docs/concepts/workloads/controllers/garbage-collection/  
https://kubernetes.io/docs/concepts/workloads/controllers/garbage-collection/#owners-and-dependents  
Explaining ownership... Deployments own ReplicaSets which own Pods.

Normally these relationships are created automatically but you can assign
them manually if you want.

https://kubernetes.io/docs/concepts/workloads/controllers/garbage-collection/#controlling-how-the-garbage-collector-deletes-dependents  
You can configure how dependent objects are deleted (cascading). Two modes
background and foreground. If you delete a parent and don't remove the
children it's called "orphaned". Yes yes, referential integrity...

https://kubernetes.io/docs/concepts/workloads/controllers/garbage-collection/#background-cascading-deletion  
Destroyed immediately + deleting dependent objects in the background. Not
sure what this is.

https://kubernetes.io/docs/concepts/workloads/controllers/garbage-collection/#foreground-cascading-deletion  
Ahh. This helps me understand what's happening in background deletion.
This flags the "root" object as having deletion in progress while all the
child elements are being removed. Seems like a relational database would
be useful here. Maybe someday cockroachdb instead of etcd?

Some outdated references to kubernetes 1.7.
https://github.com/kubernetes/website/pull/6242  

https://kubernetes.io/docs/concepts/workloads/controllers/garbage-collection/#setting-the-cascading-deletion-policy  
You can choose which policy to use. Default is orphan. Strange but I am
sure there is a good reason. Not digging into that errata now.

https://kubernetes.io/docs/concepts/workloads/controllers/garbage-collection/#additional-note-on-deployments  
Some notes that also seem outdated. Need to carry on so I am ignoring.

# 13:20
Out for a walk.

# 13:40
Back.

https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/#what-is-a-job  
Just what you'd think. Run some work, ensure it succeeds.
Can be used to run lots of jobs in parallel.

https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/#running-an-example-job  
Pretty straightforward.

https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/#writing-a-job-spec  
https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/#pod-template  
Sure.

https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/#pod-selector  
Don't use a selector here.

https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/#parallel-jobs  
Some forms...

1. One pod, one job, success, done.
2. One job, a count for N of jobs to run, success after N pods succeed.
3. Parallel jobs with a work queue - pods coordinate with external service
or themselves to determine which work to run. If any pod completes with
success no new pods will be made. When one pod ends with success and all
other pods terminate, we're successful.

https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/#controlling-parallelism  
Checks out.

https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/#handling-pod-and-container-failures  
Basically anything can fail for a myriad of reasons and your jobs need to
handle restarting in a totally new environment cleanly no matter how they
were killed.

https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/#pod-backoff-failure-policy  
Stop tryin' to do jobs that just won't succeed. Default failures is 6.
I wonder what the history of that number is.

https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/#job-patterns  
Jobs aren't meant to do closely communicating processes, they are for doing
independent but related work items. Emails. Transcoding. etc.

Some example patterns:

One Job object per work item
- clear about what it is doing
- lots of work to maintain if there are tons of jobs
One job for all work items.
- easier to scale than the previous

One pod per work item
- less modification to existing code/containers

One pod performing multiple work items
- better for larger numbers of work items

Using a work queue
- requires coordination with an external service.

https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/#advanced-usage  
Skipping this.

https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/#alternatives  
https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/#bare-pods  
https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/#replication-controller  
https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/#single-job-starts-controller-pod  
Makes sense to me.

# 14:00
Call from Mark. Going duck hunting this weekend!

# 14:30
Back to it.

https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/#what-is-a-cron-job  
https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/#prerequisites  
Check.

https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/#creating-a-cron-job  
Makes sense to me.

https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/#deleting-a-cron-job  
Makes sense to me.

https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/#cron-job-limitations  
One of the limitations is that the cron job might not start, or it might
start more than once. Good times.

https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/#writing-a-cron-job-spec  
https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/#schedule  
https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/#job-template  
https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/#starting-deadline-seconds  
https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/#concurrency-policy  
https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/#suspend  
Makes sense to me.

https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/#jobs-history-limits  
Got it.

# 14:40
Phew. Done with controllers. On to configuration!

https://kubernetes.io/docs/concepts/configuration/overview/  
Highlights and consolidates advice given throughout the docs. Great!

https://kubernetes.io/docs/concepts/configuration/overview/#general-config-tips  
- Use the latest stable api (Developers everywhere rejoice)
- Version control configs (Duh)
- Write config in YAML not JSON (Check)
- Group related objects into a single file (--- between objects)
...or group multiple related files in a directory
- Don't specify default values - minimal configs are better (disagree)
- Use an object description to make introspection easier

https://kubernetes.io/docs/concepts/configuration/overview/#naked-pods-vs-replication-controllers-and-jobs  
Don't make Pods that don't have a controller. They won't be rescheduled if
they die for some reason.

https://kubernetes.io/docs/concepts/configuration/overview/#services
Create Services before controllers. This lets the scheduler spread the Pods
that comprise the service (???)
Don't use hostPort, that limits where a Pod can be scheduled (nodes where
that port hasn't been used yet) (ok)
Avoid hostNetwork for the same reason as avoiding hostPort. (ok)
When load balancing isn't needed use headless services. (still not sure what
a headless service is)

https://kubernetes.io/docs/concepts/configuration/overview/#using-labels  
Labels are for semantic use.

Good
----
app: name
tier: frontend
phase: test
deployment: v3

Bad
---
service: someservice

- A service can span multiple deployments by omitting release specific
fields in the selector. (cool)

- Do manipulate labels for debugging. Remove one from a pod to pull it
out of rotation (the system will repair i) for inspection.

https://kubernetes.io/docs/concepts/configuration/overview/#container-images  
https://kubernetes.io/docs/concepts/configuration/overview/#using-kubectl  
Makes sense to me.

# 14:55
https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/  
Pod specs can indicate needed ram/cpu so scheduler can be more intelligent.

https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#resource-types  
CPU = cores
Memory = specified in bytes

These are "compute resources"

https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#resource-requests-and-limits-of-pod-and-container  
You can set "limits" for cpu/memory and "requests" for cpu/memory at the
pod spec level.

https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#meaning-of-cpu  
1 CPU in k8s = 1 aws vcpu / 1 gcp core / 1 azure vcore / 1 hyperthread

Fractional requests are possible.
1 = 1
1 = 1000m
0.1 = 100m
0.5 = 500m

https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#meaning-of-memory  
Tons of different forms memory can be represented in.
Example makes sense to me.

https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#how-pods-with-resource-requests-are-scheduled  
Nodes have a max capacity for each resource type.
Scheduler ensures the sum of running container needs doesn't exceed.
Even when actual usage is lower than requests the scheduler doesn't touch
a node... this leaves headroom for when it is needed.

https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#how-pods-with-resource-limits-are-run  
Requests and limits are enforced by the container runtime, as passed in by
kubelet when the container starts.

CPU requests map to docker --cpu-shares
CPU limit is the total amount of ram the container can use per 100ms
Memory requests map to docker --memory

If container exceeds memory limit, might get killed.
If container exceeds memory request entire pod might be evicted.

https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#monitoring-compute-resource-usage  
Can be seen via pod status.

https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#troubleshooting  
Pods stuck pending with failedScheduling?

https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#my-pods-are-pending-with-event-message-failedscheduling  
Scheduler can't find node where pod will fit!
Fix: clear out some Pods from Nodes you want to use or change spec.

https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#my-container-is-terminated  
Resource starvation might cause a container to be terminated. Describe the
Pod / check events to see why.

https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#local-ephemeral-storage-alpha-feature  
https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#requests-and-limits-setting-for-local-ephemeral-storage  
https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#how-pods-with-ephemeral-storage-requests-are-scheduled  
https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#how-pods-with-ephemeral-storage-limits-run  
Makes sense to me.

https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#opaque-integer-resources-alpha-feature  
Deprecated feature. Ignoring.

https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#extended-resources  
This allows cluster managers to advertise resource limits that are not cpu
or memory?

Advertising a resource limit is accomplished by sending a patch request
to the node `status.capacity` field. k8s will maintain the corresponding
usage under the nodes `status.allocatable` field.

https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#planned-improvements  
Check.

# 15:15
https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
You can limit Pods to specific Nodes, but you generally shouldn't have to
unless you have very specific constraints, like, run this work on a Node
that has access to a SSD volume, or, run these Pods on Nodes in the same
az, etc.

https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector  
https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#step-zero-prerequisites  
https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#step-one-attach-label-to-the-node  
https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#step-two-add-a-nodeselector-field  
https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#interlude-built-in-node-labels  
Check.

If you want to restrict Pods to run on specific Nodes by label, add a label
to a Node, add a selector to a Pod spec and go. There are some labels that
come with Nodes by default
- kubernetes.io/hostname
- failure-domain.beta.kubernetes.io/zone
- failure-domain.beta.kubernetes.io/region
- beta.kubernetes.io/instance-type
- beta.kubernetes.io/os
- beta.kubernetes.io/arch

https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity  
https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-affinity-beta-feature  
This whole feature set lets you say how badly you want a Pod be scheduled
on a specific type of Node... less constrained version of nodeSelector.

https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity-beta-feature  
https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#an-example-of-a-pod-that-uses-pod-affinity  
This does the same as the previous but it isn't based on the Node but rather
what Pods are running on them! Neat.

https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#more-practical-use-cases  
https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#always-co-located-in-the-same-node  
https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#never-co-located-in-the-same-node  
Shows setting up pod-based affinity to ensure a cache always ends up next  
to a webserver. Neat.

# 15:30
https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/  
Taints are the same concept as affinity, but in the opposing manner.
Taints go on nodes. Tolerations go on pods.

...need a break.

# 15:35
Talking w/ @mattsurabian.

# 16:00
Skipping the taints and tolerations section until I need it. I feel
sufficiently aware of the feature and what it can do.

On to my favorite (groan) subject, secrets.

https://kubernetes.io/docs/concepts/configuration/secret/  
https://kubernetes.io/docs/concepts/configuration/secret/#overview-of-secrets  
Secrets are just like ConfigMaps but encrypted.

https://kubernetes.io/docs/concepts/configuration/secret/#built-in-secrets  
https://kubernetes.io/docs/concepts/configuration/secret/#service-accounts-automatically-create-and-attach-secrets-with-api-credentials  
Kubernetes is already using secrets at the Pod level for mediating access
to the api server.

https://kubernetes.io/docs/concepts/configuration/secret/#creating-your-own-secrets  
https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-using-kubectl-create-secret  
Showing how to make a secret using kubectl. Same as ConfigMap.

https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually  
Showing how to create a yaml file with secret data in it. The secret values
must be base64 encoded. Not sure why, yet.

https://kubernetes.io/docs/concepts/configuration/secret/#decoding-a-secret  
Shows how to use kubectl to fetch the secret and base64 decode it.
...why all the base64 encoding?? I know that for a long time k8s didn't
encrypt secrets at all. Is this old documentation?

https://kubernetes.io/docs/concepts/configuration/secret/#using-secrets  
https://kubernetes.io/docs/concepts/configuration/secret/#using-secrets-as-files-from-a-pod  
https://kubernetes.io/docs/concepts/configuration/secret/#using-secrets-as-environment-variables  
Basically the same usage patterns as ConfigMap.

https://kubernetes.io/docs/concepts/configuration/secret/#using-imagepullsecrets  
https://kubernetes.io/docs/concepts/configuration/secret/#automatic-mounting-of-manually-created-secrets  
It is possible to inject secrets into every Pod using service accounts.

https://kubernetes.io/docs/concepts/configuration/secret/#details  
https://kubernetes.io/docs/concepts/configuration/secret/#restrictions  
Secrets are validated to exist before they are used. Pods can't reference
secrets that don't exist (Is it different for ConfigMaps?)
Don't make huge secrets, they are bounded to 1MB.

https://kubernetes.io/docs/concepts/configuration/secret/#secret-and-pod-lifetime-interaction  
If a Pod is created manually via the API no check for secret existence is
performed.

https://kubernetes.io/docs/concepts/configuration/secret/#use-cases  
https://kubernetes.io/docs/concepts/configuration/secret/#use-case-pod-with-ssh-keys  
https://kubernetes.io/docs/concepts/configuration/secret/#use-case-pods-with-prod--test-credentials  
https://kubernetes.io/docs/concepts/configuration/secret/#use-case-dotfiles-in-secret-volume  
https://kubernetes.io/docs/concepts/configuration/secret/#use-case-secret-visible-to-one-container-in-a-pod  
Some basic examples of how to create and consume secrets. Makes sense to me.

https://kubernetes.io/docs/concepts/configuration/secret/#best-practices  
https://kubernetes.io/docs/concepts/configuration/secret/#clients-that-use-the-secrets-api  
Prevent watch and list actions on secret namespaces.
Allow applications to get requests for specific secrets using RBAC.

https://kubernetes.io/docs/concepts/configuration/secret/#security-properties  
https://kubernetes.io/docs/concepts/configuration/secret/#protections  
A secret is only sent to a Node if requested. It's stored in a tempfs.
When the pod is deleted, the secret (and tempfs) is deleted.
Transmitting from the api to the kubelet to the pod is all TLS.
Each container in a Pod must request secrets, they are not shared.

https://kubernetes.io/docs/concepts/configuration/secret/#risks  
Uh. Secrets are not secure in any way, really.

I thought I read you could encrypt them at rest though...

...yeah that's referenced here:
https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/  

PR to add that. Seems very relevant:
https://github.com/kubernetes/website/pull/6245/files  

# 16:35
https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/  
kubectl looks for $HOME/.kube/config
Can be overridden by KUBECONFIG environment variable or --kubeconfig flag.

https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/#supporting-multiple-clusters-users-and-authentication-mechanisms  
Makes sense to me.

https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/#context  
Makes sense to me.

https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/#the-kubeconfig-environment-variable  
https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/#merging-kubeconfig-files  
Woof, you can merge config files. Knowing these rules is enough. I hope I
never have to use them.

https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/#file-references  
References to external files in the kube config are relative to the config
file itself. Check.

https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/  
Skipping for now.

# 16:40
https://kubernetes.io/docs/concepts/services-networking/service/  
This is review of stuff I know at this point. Pods come, Pods go. Services
make them visible.

https://kubernetes.io/docs/concepts/services-networking/service/#defining-a-service  
Whoever wrote this doc is trying to explain the entire kubernetes system
while also explaining services. Happily, I understand enough now to follow
this.

https://kubernetes.io/docs/concepts/services-networking/service/#services-without-selectors  
New concept I just learned, Endpoints are a thing. Services abstract over
them. The Service defines which Pods should be selected and the Endpoints
needed to create the mapping are managed by k8s. If you don't have any
Pod selectors, you can create endpoints yourself to point to things outside
of k8s. Neat.

It's also possible to create a service that creates Endpoints even though
they don't point to Pods. That's possible thusly:
```
kind: Service
apiVersion: v1
metadata:
name: my-service
namespace: prod
spec:
type: ExternalName
externalName: my.database.example.com
```

When looking up the host (my-service.prod.svc.CLUSTER), the cluster dns
service will give back a CNAME to my.database.example.com. So, redirection
happens at the DNS level rather than proxying/port forwarding.

https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies  
Every Node in k8s runs kube-proxy, which orchestrates iptables.

https://kubernetes.io/docs/concepts/services-networking/service/#proxy-mode-userspace  
In this mode kube-proxy watches for addition and removal of Service and
Endpoint objects. For each service it randomly opens a port on the Node it
controls. Any connections on this port will be sent to one of the pods
controlled by the Service (mediated by Endpoints). This is possible due
to iptables rules that point to the proxy, which in turn route to pods.

# 17:20
Losing focus. Calling it a night.
