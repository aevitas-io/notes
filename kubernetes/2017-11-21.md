# 22:50
Got my first etcd cluster running on AWS today. Getting close.

Also automated all of the PKI scripting with the help of this repo:
https://github.com/kelseyhightower/kubernetes-the-hard-way  

Going to read all the etcd docs now. Let's see if I can do it in a single
sitting. Surely it'll be shorter than k8s was...

https://coreos.com/etcd/docs/latest/learning/data_model.html  
Meant to reliably store infrequently updated data and notifications on
changes. Also designed to support versioning. The store is immutable. All
historical changes are accessible unless configured to be purged.

https://coreos.com/etcd/docs/latest/dl_build.html  
Recommended to run on a "medium" instance in AWS or a "standard-1" instance
on GCE.

https://coreos.com/etcd/docs/latest/op-guide/supported-platform.html  
etcd is supported by core maintainers on amd64 darwin and linux and ppc64le
linux.

https://coreos.com/etcd/docs/latest/dev-guide/limit.html  
Request size is assumed to be less than 1mb. Storage size defaults to 2gb
max.

https://coreos.com/etcd/docs/latest/op-guide/hardware.html  
CPU
2 to 4 cores is typically plenty. Serving thousands of clients or tens of
thousands of requests per second, tend to be CPU bound since etcd can serve
requests from memory. Expect 8-16 cores to service that.

RAM
Mostly consumed by aggressively caching data and tracking watchers.
Typically 8GB is enough. Thousands of watchers / millions of keys need
16GB-64GB.

Disks
Most critical. Raft requires persisting data to a log. Majority of members
must write request to disk. Incremental checkpoints of memory state also
require writing to disk. Writes taking too long can time out heartbeats.

Very sensitive to disk write latency. 50 sequential IOPS required. 500 for
heavily loaded servers. Most important when in recovery.

Network
1GbE is sufficient for common deployments.

https://coreos.com/etcd/docs/latest/tuning.html  

Time parameters
Raft heartbeat should be ~1.5x RTT between members.
Raft election timeout should be ~10x heartbeat

Snapshots
Defaults to complete linear history of every change.
To avoid having a huge log, etcd sometimes makes snapshots.

Snapshot tuning
Every 10k writes is the default for making a snapshot.

Disk
Sensitive to disk latency. Can be run alongside other processes
sometimes if given high disk priority.

Network
When leader is serving read heavy loads. On Linux this can be mitigated by
prioritizing peer traffic using `tc`.

https://coreos.com/etcd/docs/latest/op-guide/performance.html  
A standard n-4 instance on google with a three member etcd cluster finishes
a request in less than one millisecond under light load, and can complete
more than 30,000 requests per second under heavy load. Note: this would
cost ~$3k/month...

Etcd uses Raft. Min time to complete is network round trip plus fdatasync
time to commit to storage.

Benchmarks
A benchmarking cli is included w/ etcd.

https://coreos.com/etcd/docs/latest/op-guide/gateway.html  
Not needed for k8s.

https://coreos.com/etcd/docs/latest/op-guide/grpc_proxy.html  
Manages watch/lease API requests.

Scalable watch API
Coalesces multiple watchers on the same key to reduce load on etcd from N to
1.

Limitations
Watch coalescing can result in slight revision drift if the watcher does not
specify starting the watch at a specific revision. Practically speaking
this usually doesn't matter.

Scalable lease API
This does the same thing as the watch API, but for gRPC streams.

https://coreos.com/etcd/docs/latest/dev-guide/api_grpc_gateway.html  
etcd v3 uses gRPC. For clients who don't support gRPC, a JSON gateway
is provided that handles the conversion.

https://coreos.com/etcd/docs/latest/dev-guide/local_cluster.html  
Simple examples for running etcd locally.

https://coreos.com/etcd/docs/latest/op-guide/clustering.html  
https://coreos.com/etcd/docs/latest/op-guide/clustering.html#static  
Makes sense to me, this is how I tested today.

https://coreos.com/etcd/docs/latest/op-guide/clustering.html#tls  
Setup follows what I did today with the exception that it is recommended
to have a separate key for each member. "Auto" TLS is allowed too, where
traffic is encrypted but not authenticated.

Discovery
Can be done using a discovery service...

Or DNS SRV records.

Not going to do this.

https://coreos.com/etcd/docs/latest/dev-guide/interacting_v3.html  
Basic command line usage. Roger.

https://coreos.com/etcd/docs/latest/op-guide/configuration.html  
Each command line option is nicely defined.

https://coreos.com/etcd/docs/latest/op-guide/security.html  

https://coreos.com/etcd/docs/latest/etcd-live-http-to-https-migration.html  
Skipping. I'll always start with encrypted comms.

Good enough for now.
